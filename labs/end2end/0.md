# End-to-end Machine Learning with Tensorflow on GCP

## Overview

In this workshop, we walk through the process of building a complete machine learning pipeline covering ingest, exploration, training, evaluation, deployment, and prediction. Along the way, we will discuss how to explore and split large data sets correctly using BigQuery and Cloud Datalab. The machine learning model in TensorFlow will be developed on a small sample locally. The preprocessing operations will be implemented in Cloud Dataflow, so that the same preprocessing can be applied in streaming mode as well. The training of the model will then be distributed and scaled out on Cloud ML Engine. The trained model will be deployed as a microservice and predictions invoked from a web application.

This lab consists of 7 parts and will take you about 3 hours. It goes along with this [slide deck][1].

## **What you need**

To complete this lab, you need:

* A project and bucket on Google Cloud Platform

## **What you learn**

In this lab, you:

* Explore a large dataset using Datalab and BigQuery
* Export data for machine learning using Cloud Dataflow
* Develop a machine learning model in Tensorflow
* Train a machine learning model at scale on Cloud ML Engine
* Deploy the trained ML model as a microservice
* Invoke the trained model from an AppEngine web application and a Java Dataflow pipeline.



